"""
Test Suite pour valider le r√¥le du Data Officer
V√©rifie que tous les composants de t√©l√©m√©trie fonctionnent correctement
"""
import sys
import json
from pathlib import Path

# Ajouter src au path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from utils.logger import log_experiment, ActionType, initialize_logger, finalize_logger
from tools.telemetry import TelemetryTracker, EventType
from tools.data_validator import DataValidator
from tools.metrics_analyzer import MetricsAnalyzer


class TestDataOfficerRole:
    """Tests pour valider le r√¥le du Data Officer"""
    
    def __init__(self, test_dir: Path):
        self.test_dir = test_dir
        self.test_dir.mkdir(parents=True, exist_ok=True)
        self.results = {
            "tests_passed": 0,
            "tests_failed": 0,
            "errors": []
        }
    
    def run_all_tests(self):
        """Ex√©cute tous les tests"""
        print("=" * 70)
        print("üß™ TEST SUITE - R√îLE DATA OFFICER")
        print("=" * 70)
        print()
        
        tests = [
            ("Test 1: Logger impos√© existe", self.test_logger_exists),
            ("Test 2: ActionType conforme", self.test_action_type_enum),
            ("Test 3: Validation champs obligatoires", self.test_required_fields),
            ("Test 4: G√©n√©ration experiment_data.json", self.test_json_generation),
            ("Test 5: Validation du sch√©ma JSON", self.test_json_schema_validation),
            ("Test 6: TelemetryTracker compatible", self.test_telemetry_integration),
            ("Test 7: MetricsAnalyzer fonctionnel", self.test_metrics_analyzer),
            ("Test 8: Dataset de test g√©n√©rable", self.test_dataset_generation),
        ]
        
        for test_name, test_func in tests:
            try:
                test_func()
                self._mark_success(test_name)
            except Exception as e:
                self._mark_failure(test_name, str(e))
        
        self._print_summary()
        return self.results["tests_failed"] == 0
    
    def test_logger_exists(self):
        """V√©rifie que le logger impos√© existe avec la bonne structure"""
        from utils import logger
        
        # V√©rifier les exports obligatoires
        assert hasattr(logger, 'log_experiment'), "Fonction log_experiment manquante"
        assert hasattr(logger, 'ActionType'), "Enum ActionType manquante"
        assert hasattr(logger, 'initialize_logger'), "Fonction initialize_logger manquante"
        assert hasattr(logger, 'finalize_logger'), "Fonction finalize_logger manquante"
    
    def test_action_type_enum(self):
        """V√©rifie que ActionType a les valeurs impos√©es"""
        required_actions = ['ANALYSIS', 'GENERATION', 'DEBUG', 'FIX']
        
        for action in required_actions:
            assert hasattr(ActionType, action), f"ActionType.{action} manquant"
            assert ActionType[action].value == action.lower(), f"Valeur incorrecte pour {action}"
    
    def test_required_fields(self):
        """Teste la validation des champs obligatoires"""
        log_dir = self.test_dir / "test_required_fields"
        initialize_logger(log_dir)
        
        # Test 1: Doit √©chouer sans input_prompt
        try:
            log_experiment(
                agent_name="TestAgent",
                model_used="test-model",
                action=ActionType.ANALYSIS,
                details={"output_response": "test"},
                status="SUCCESS"
            )
            raise AssertionError("Devrait √©chouer sans input_prompt")
        except ValueError as e:
            assert "input_prompt" in str(e), "Message d'erreur incorrect"
        
        # Test 2: Doit √©chouer sans output_response
        try:
            log_experiment(
                agent_name="TestAgent",
                model_used="test-model",
                action=ActionType.ANALYSIS,
                details={"input_prompt": "test"},
                status="SUCCESS"
            )
            raise AssertionError("Devrait √©chouer sans output_response")
        except ValueError as e:
            assert "output_response" in str(e), "Message d'erreur incorrect"
        
        # Test 3: Doit r√©ussir avec les deux champs
        log_experiment(
            agent_name="TestAgent",
            model_used="test-model",
            action=ActionType.ANALYSIS,
            details={
                "input_prompt": "Test prompt",
                "output_response": "Test response"
            },
            status="SUCCESS"
        )
    
    def test_json_generation(self):
        """Teste la g√©n√©ration du fichier experiment_data.json"""
        log_dir = self.test_dir / "test_json_generation"
        initialize_logger(log_dir)
        
        # G√©n√©rer quelques logs
        for i in range(3):
            log_experiment(
                agent_name=f"Agent_{i}",
                model_used="gemini-2.5-flash",
                action=ActionType.ANALYSIS,
                details={
                    "input_prompt": f"Prompt {i}",
                    "output_response": f"Response {i}",
                    "iteration": i
                },
                status="SUCCESS"
            )
        
        finalize_logger()
        
        # V√©rifier que le fichier existe
        json_file = log_dir / "experiment_data.json"
        assert json_file.exists(), "experiment_data.json n'a pas √©t√© cr√©√©"
        
        # V√©rifier que c'est du JSON valide
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # V√©rifier la structure
        assert "session_id" in data, "session_id manquant"
        assert "logs" in data, "logs manquant"
        assert len(data["logs"]) == 3, f"Nombre de logs incorrect: {len(data['logs'])}"
    
    def test_json_schema_validation(self):
        """Teste la validation du sch√©ma avec DataValidator"""
        log_dir = self.test_dir / "test_validation"
        initialize_logger(log_dir)
        
        # Cr√©er des logs valides
        log_experiment(
            agent_name="Auditor_Agent",
            model_used="gemini-2.5-flash",
            action=ActionType.ANALYSIS,
            details={
                "input_prompt": "Analyse ce code",
                "output_response": "Code analys√©",
                "file": "test.py"
            },
            status="SUCCESS"
        )
        
        finalize_logger()
        
        # Valider avec DataValidator
        json_file = log_dir / "experiment_data.json"
        is_valid, errors = DataValidator.validate_file(json_file)
        
        if not is_valid:
            print(f"‚ùå Erreurs de validation: {errors}")
        
        assert is_valid, f"Validation √©chou√©e: {errors}"
    
    def test_telemetry_integration(self):
        """Teste l'int√©gration avec TelemetryTracker"""
        log_dir = self.test_dir / "test_telemetry"
        tracker = TelemetryTracker()
        tracker.initialize(log_dir)
        
        # Utiliser le tracker
        tracker.track_event(
            event_type=EventType.CODE_ANALYSIS,
            agent_name="TestAgent",
            data={
                "input_prompt": "Test",
                "output_response": "Result",
                "file": "test.py"
            },
            success=True
        )
        
        tracker.finalize()
        
        # V√©rifier que les fichiers sont cr√©√©s
        assert (log_dir / "telemetry_data.json").exists(), "telemetry_data.json manquant"
    
    def test_metrics_analyzer(self):
        """Teste l'analyseur de m√©triques"""
        log_dir = self.test_dir / "test_metrics"
        initialize_logger(log_dir)
        
        # Cr√©er des logs vari√©s
        actions = [ActionType.ANALYSIS, ActionType.FIX, ActionType.DEBUG]
        agents = ["Auditor", "Fixer", "Judge"]
        
        for i, (action, agent) in enumerate(zip(actions, agents)):
            log_experiment(
                agent_name=agent,
                model_used="gemini-2.5-flash",
                action=action,
                details={
                    "input_prompt": f"Prompt {i}",
                    "output_response": f"Response {i}"
                },
                status="SUCCESS" if i % 2 == 0 else "FAILURE"
            )
        
        finalize_logger()
        
        # Analyser
        json_file = log_dir / "experiment_data.json"
        analyzer = MetricsAnalyzer(json_file)
        
        # V√©rifier les m√©triques
        agent_perf = analyzer.get_agent_performance()
        assert len(agent_perf) == 3, "Nombre d'agents incorrect"
        
        report = analyzer.generate_summary_report()
        assert "RAPPORT D'ANALYSE" in report, "Rapport mal format√©"
    
    def test_dataset_generation(self):
        """Teste la g√©n√©ration du dataset de test"""
        from tests.test_dataset_generator import TestDatasetGenerator
        
        output_dir = self.test_dir / "test_dataset"
        TestDatasetGenerator.generate_dataset(output_dir, num_cases=3)
        
        # V√©rifier la structure
        assert output_dir.exists(), "Dataset non cr√©√©"
        assert (output_dir / "index.json").exists(), "index.json manquant"
        
        # V√©rifier qu'il y a des cas
        cases = list(output_dir.glob("case*"))
        assert len(cases) == 3, f"Nombre de cas incorrect: {len(cases)}"
        
        # V√©rifier la structure d'un cas
        first_case = cases[0]
        assert (first_case / "buggy_code.py").exists(), "Code bugg√© manquant"
        assert (first_case / "metadata.json").exists(), "Metadata manquante"
    
    def _mark_success(self, test_name: str):
        """Marque un test comme r√©ussi"""
        self.results["tests_passed"] += 1
        print(f"‚úÖ {test_name}")
    
    def _mark_failure(self, test_name: str, error: str):
        """Marque un test comme √©chou√©"""
        self.results["tests_failed"] += 1
        self.results["errors"].append({"test": test_name, "error": error})
        print(f"‚ùå {test_name}")
        print(f"   Erreur: {error}")
    
    def _print_summary(self):
        """Affiche le r√©sum√© des tests"""
        print()
        print("=" * 70)
        print("üìä R√âSUM√â DES TESTS")
        print("=" * 70)
        print(f"‚úÖ Tests r√©ussis: {self.results['tests_passed']}")
        print(f"‚ùå Tests √©chou√©s: {self.results['tests_failed']}")
        print()
        
        if self.results["tests_failed"] == 0:
            print("üéâ TOUS LES TESTS SONT PASS√âS!")
            print("‚úÖ Votre r√¥le de Data Officer est correctement impl√©ment√©.")
        else:
            print("‚ö†Ô∏è  CERTAINS TESTS ONT √âCHOU√â")
            print("Erreurs d√©taill√©es:")
            for error in self.results["errors"]:
                print(f"  - {error['test']}: {error['error']}")
        
        print("=" * 70)


def main():
    """Point d'entr√©e principal"""
    test_dir = Path("test_output_data_officer")
    
    # Nettoyer le dossier de test
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
    
    # Ex√©cuter les tests
    tester = TestDataOfficerRole(test_dir)
    success = tester.run_all_tests()
    
    # Code de sortie
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
