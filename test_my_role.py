"""
Script de test complet pour le rÃ´le Data Officer
Lance tous les tests pour valider que votre implÃ©mentation fonctionne
"""
import sys
import json
import shutil
from pathlib import Path

# Ajouter src au path AVANT tout import
base_path = Path(__file__).parent / "src"
if str(base_path) not in sys.path:
    sys.path.insert(0, str(base_path))

# Imports directs maintenant
from utils.logger import log_experiment, ActionType, initialize_logger, finalize_logger, get_logger_stats

# Import avec gestion d'erreur pour telemetry
try:
    from tools.telemetry import TelemetryTracker, EventType
except ImportError as e:
    print(f"âš ï¸  Import telemetry Ã©chouÃ©, chargement direct...")
    import importlib.util
    telemetry_path = Path(__file__).parent / "src" / "tools" / "telemetry.py"
    spec = importlib.util.spec_from_file_location("telemetry", telemetry_path)
    telemetry_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(telemetry_module)
    TelemetryTracker = telemetry_module.TelemetryTracker
    EventType = telemetry_module.EventType

from tools.data_validator import DataValidator
from tools.metrics_analyzer import MetricsAnalyzer


def test_1_logger_base():
    """Test 1 : Fonctionnement de base du logger"""
    print("\nğŸ§ª Test 1 : Logger de base")
    print("-" * 60)
    
    test_dir = Path("test_outputs/test1_logger")
    if test_dir.exists():
        shutil.rmtree(test_dir)
    
    initialize_logger(test_dir)
    
    # Log simple
    log_experiment(
        agent_name="TestAgent",
        model_used="gemini-2.5-flash",
        action=ActionType.ANALYSIS,
        details={
            "input_prompt": "Analyse ce code Python",
            "output_response": "Le code contient 3 problÃ¨mes",
            "file": "test.py"
        },
        status="SUCCESS"
    )
    
    finalize_logger()
    
    # VÃ©rification
    json_file = test_dir / "experiment_data.json"
    assert json_file.exists(), "âŒ experiment_data.json non crÃ©Ã©"
    
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    assert len(data["logs"]) == 1, "âŒ Nombre de logs incorrect"
    assert data["logs"][0]["action"] == "analysis", "âŒ Type d'action incorrect"
    
    print("âœ… Logger de base fonctionne parfaitement")


def test_2_validation_champs_obligatoires():
    """Test 2 : Validation des champs obligatoires"""
    print("\nğŸ§ª Test 2 : Validation champs obligatoires")
    print("-" * 60)
    
    test_dir = Path("test_outputs/test2_validation")
    if test_dir.exists():
        shutil.rmtree(test_dir)
    
    initialize_logger(test_dir)
    
    # Test sans input_prompt (doit Ã©chouer)
    try:
        log_experiment(
            agent_name="TestAgent",
            model_used="test",
            action=ActionType.FIX,
            details={"output_response": "test"},
            status="SUCCESS"
        )
        print("âŒ ERREUR : Devrait Ã©chouer sans input_prompt")
        return False
    except ValueError as e:
        if "input_prompt" in str(e):
            print("âœ… Validation input_prompt OK")
        else:
            print(f"âŒ Mauvais message d'erreur: {e}")
            return False
    
    # Test sans output_response (doit Ã©chouer)
    try:
        log_experiment(
            agent_name="TestAgent",
            model_used="test",
            action=ActionType.FIX,
            details={"input_prompt": "test"},
            status="SUCCESS"
        )
        print("âŒ ERREUR : Devrait Ã©chouer sans output_response")
        return False
    except ValueError as e:
        if "output_response" in str(e):
            print("âœ… Validation output_response OK")
        else:
            print(f"âŒ Mauvais message d'erreur: {e}")
            return False
    
    print("âœ… Validation des champs obligatoires fonctionne")


def test_3_les_4_action_types():
    """Test 3 : Les 4 ActionType imposÃ©s"""
    print("\nğŸ§ª Test 3 : Les 4 ActionType imposÃ©s")
    print("-" * 60)
    
    test_dir = Path("test_outputs/test3_actions")
    if test_dir.exists():
        shutil.rmtree(test_dir)
    
    initialize_logger(test_dir)
    
    actions_a_tester = [
        (ActionType.ANALYSIS, "Analyse du code"),
        (ActionType.GENERATION, "GÃ©nÃ©ration de tests"),
        (ActionType.DEBUG, "Debug d'une erreur"),
        (ActionType.FIX, "Correction du code")
    ]
    
    for action, description in actions_a_tester:
        log_experiment(
            agent_name="MultiAgent",
            model_used="gemini-2.5-flash",
            action=action,
            details={
                "input_prompt": f"Prompt pour {description}",
                "output_response": f"RÃ©sultat de {description}",
                "task": description
            },
            status="SUCCESS"
        )
        print(f"  âœ“ {action.value} testÃ©")
    
    finalize_logger()
    
    # VÃ©rifier le fichier
    with open(test_dir / "experiment_data.json", 'r') as f:
        data = json.load(f)
    
    actions_logged = [log["action"] for log in data["logs"]]
    assert "analysis" in actions_logged, "âŒ ANALYSIS manquant"
    assert "generation" in actions_logged, "âŒ GENERATION manquant"
    assert "debug" in actions_logged, "âŒ DEBUG manquant"
    assert "fix" in actions_logged, "âŒ FIX manquant"
    
    print("âœ… Les 4 ActionType fonctionnent correctement")


def test_4_telemetry_tracker():
    """Test 4 : TelemetryTracker compatible"""
    print("\nğŸ§ª Test 4 : TelemetryTracker")
    print("-" * 60)
    
    test_dir = Path("test_outputs/test4_telemetry")
    if test_dir.exists():
        shutil.rmtree(test_dir)
    
    initialize_logger(test_dir)
    
    tracker = TelemetryTracker()
    tracker.initialize(test_dir)
    
    # Tracker quelques Ã©vÃ©nements
    tracker.start_iteration(1)
    
    tracker.track_event(
        event_type=EventType.CODE_ANALYSIS,
        agent_name="Auditor",
        data={
            "input_prompt": "Analyse le fichier buggy.py",
            "output_response": "TrouvÃ© 5 problÃ¨mes",
            "file": "buggy.py"
        },
        duration_ms=150.5
    )
    
    tracker.track_event(
        event_type=EventType.CODE_MODIFICATION,
        agent_name="Fixer",
        data={
            "input_prompt": "Corrige les 5 problÃ¨mes",
            "output_response": "Corrections appliquÃ©es",
            "file": "buggy.py"
        },
        duration_ms=320.8
    )
    
    tracker.end_iteration(1, success=True)
    tracker.finalize()
    
    # VÃ©rifications
    telemetry_file = test_dir / "telemetry_data.json"
    experiment_file = test_dir / "experiment_data.json"
    
    assert telemetry_file.exists(), "âŒ telemetry_data.json manquant"
    assert experiment_file.exists(), "âŒ experiment_data.json manquant"
    
    print("  âœ“ Les deux fichiers JSON crÃ©Ã©s")
    
    # VÃ©rifier que les logs LLM sont enregistrÃ©s
    with open(experiment_file, 'r') as f:
        exp_data = json.load(f)
    
    # Doit avoir au moins 2 logs (CODE_ANALYSIS et CODE_MODIFICATION)
    assert len(exp_data["logs"]) >= 2, f"âŒ Pas assez de logs: {len(exp_data['logs'])}"
    
    print("âœ… TelemetryTracker compatible avec le logger imposÃ©")


def test_5_data_validator():
    """Test 5 : DataValidator"""
    print("\nğŸ§ª Test 5 : DataValidator")
    print("-" * 60)
    
    test_dir = Path("test_outputs/test5_validator")
    if test_dir.exists():
        shutil.rmtree(test_dir)
    
    initialize_logger(test_dir)
    
    # CrÃ©er des logs valides
    for i in range(3):
        log_experiment(
            agent_name=f"Agent_{i}",
            model_used="gemini-2.5-flash",
            action=ActionType.ANALYSIS,
            details={
                "input_prompt": f"Prompt {i}",
                "output_response": f"Response {i}",
                "iteration": i
            },
            status="SUCCESS"
        )
    
    finalize_logger()
    
    # Valider
    json_file = test_dir / "experiment_data.json"
    is_valid, errors = DataValidator.validate_file(json_file)
    
    if not is_valid:
        print(f"âŒ Validation Ã©chouÃ©e: {errors}")
        return False
    
    print("  âœ“ Fichier JSON validÃ© avec succÃ¨s")
    
    # GÃ©nÃ©rer un rapport
    report = DataValidator.generate_report(json_file)
    assert "VALIDATION RÃ‰USSIE" in report, "âŒ Rapport incorrect"
    
    print("âœ… DataValidator fonctionne correctement")


def test_6_metrics_analyzer():
    """Test 6 : MetricsAnalyzer"""
    print("\nğŸ§ª Test 6 : MetricsAnalyzer")
    print("-" * 60)
    
    test_dir = Path("test_outputs/test6_metrics")
    if test_dir.exists():
        shutil.rmtree(test_dir)
    
    initialize_logger(test_dir)
    
    # CrÃ©er des logs variÃ©s
    agents = ["Auditor", "Fixer", "Judge"]
    actions = [ActionType.ANALYSIS, ActionType.FIX, ActionType.DEBUG]
    
    for i, (agent, action) in enumerate(zip(agents, actions)):
        status = "SUCCESS" if i % 2 == 0 else "FAILURE"
        log_experiment(
            agent_name=agent,
            model_used="gemini-2.5-flash",
            action=action,
            details={
                "input_prompt": f"Task {i}",
                "output_response": f"Result {i}",
                "iteration": i
            },
            status=status
        )
    
    finalize_logger()
    
    # Analyser
    json_file = test_dir / "experiment_data.json"
    analyzer = MetricsAnalyzer(json_file)
    
    # Tester les mÃ©triques
    agent_perf = analyzer.get_agent_performance()
    assert len(agent_perf) == 3, f"âŒ Nombre d'agents incorrect: {len(agent_perf)}"
    print(f"  âœ“ {len(agent_perf)} agents analysÃ©s")
    
    # Tester le rapport
    report = analyzer.generate_summary_report()
    assert "RAPPORT D'ANALYSE" in report, "âŒ Rapport mal formatÃ©"
    print("  âœ“ Rapport gÃ©nÃ©rÃ©")
    
    print("âœ… MetricsAnalyzer fonctionne correctement")


def test_7_scenario_complet():
    """Test 7 : ScÃ©nario complet de refactoring"""
    print("\nğŸ§ª Test 7 : ScÃ©nario complet (simulation rÃ©elle)")
    print("-" * 60)
    
    test_dir = Path("test_outputs/test7_scenario")
    if test_dir.exists():
        shutil.rmtree(test_dir)
    
    initialize_logger(test_dir)
    tracker = TelemetryTracker()
    tracker.initialize(test_dir)
    
    # Simulation d'un workflow complet
    print("  ğŸ“ ItÃ©ration 1 : Analyse initiale")
    tracker.start_iteration(1)
    
    log_experiment(
        agent_name="Auditor_Agent",
        model_used="gemini-2.5-flash",
        action=ActionType.ANALYSIS,
        details={
            "input_prompt": "Analyse le code dans buggy_code.py et identifie tous les problÃ¨mes",
            "output_response": "J'ai identifiÃ© 5 problÃ¨mes: 1) Pas de docstrings, 2) Variables non dÃ©finies...",
            "file": "buggy_code.py",
            "issues_found": 5,
            "pylint_score": 3.2
        },
        status="SUCCESS"
    )
    
    print("  ğŸ”§ ItÃ©ration 1 : Corrections")
    log_experiment(
        agent_name="Fixer_Agent",
        model_used="gemini-2.5-flash",
        action=ActionType.FIX,
        details={
            "input_prompt": "Corrige les 5 problÃ¨mes identifiÃ©s dans buggy_code.py",
            "output_response": "J'ai corrigÃ© le code en ajoutant des docstrings et en dÃ©finissant les variables",
            "file": "buggy_code.py",
            "changes_made": 5
        },
        status="SUCCESS"
    )
    
    print("  ğŸ§ª ItÃ©ration 1 : Tests")
    log_experiment(
        agent_name="Judge_Agent",
        model_used="gemini-2.5-flash",
        action=ActionType.DEBUG,
        details={
            "input_prompt": "ExÃ©cute les tests unitaires sur buggy_code.py",
            "output_response": "2 tests passent, 1 test Ã©choue",
            "file": "buggy_code.py",
            "tests_passed": 2,
            "tests_failed": 1
        },
        status="FAILURE"
    )
    
    tracker.end_iteration(1, success=False)
    
    print("  ğŸ“ ItÃ©ration 2 : Nouvelle correction")
    tracker.start_iteration(2)
    
    log_experiment(
        agent_name="Fixer_Agent",
        model_used="gemini-2.5-flash",
        action=ActionType.FIX,
        details={
            "input_prompt": "Corrige l'erreur qui fait Ã©chouer le test",
            "output_response": "Erreur corrigÃ©e: mauvais calcul dans la fonction sum",
            "file": "buggy_code.py",
            "changes_made": 1
        },
        status="SUCCESS"
    )
    
    log_experiment(
        agent_name="Judge_Agent",
        model_used="gemini-2.5-flash",
        action=ActionType.DEBUG,
        details={
            "input_prompt": "ExÃ©cute les tests unitaires",
            "output_response": "Tous les tests passent!",
            "file": "buggy_code.py",
            "tests_passed": 3,
            "tests_failed": 0
        },
        status="SUCCESS"
    )
    
    tracker.end_iteration(2, success=True)
    
    finalize_logger()
    tracker.finalize()
    
    print("\n  ğŸ“Š Analyse des rÃ©sultats...")
    
    # Analyser avec MetricsAnalyzer
    json_file = test_dir / "experiment_data.json"
    analyzer = MetricsAnalyzer(json_file)
    
    stats = get_logger_stats()
    print(f"    â€¢ Total logs: {stats['total_logs']}")
    print(f"    â€¢ SuccÃ¨s: {stats['success_count']}")
    print(f"    â€¢ Ã‰checs: {stats['failure_count']}")
    print(f"    â€¢ Agents: {', '.join(stats['agents'])}")
    
    # Valider
    is_valid, errors = DataValidator.validate_file(json_file)
    if is_valid:
        print("    â€¢ Validation: âœ“ OK")
    else:
        print(f"    â€¢ Validation: âœ— Erreurs: {errors}")
        return False
    
    print("\nâœ… ScÃ©nario complet rÃ©ussi - SystÃ¨me prÃªt pour production!")


def main():
    """Lance tous les tests"""
    print("=" * 70)
    print("ğŸš€ TEST COMPLET DU RÃ”LE DATA OFFICER")
    print("=" * 70)
    
    tests = [
        test_1_logger_base,
        test_2_validation_champs_obligatoires,
        test_3_les_4_action_types,
        test_4_telemetry_tracker,
        test_5_data_validator,
        test_6_metrics_analyzer,
        test_7_scenario_complet
    ]
    
    passed = 0
    failed = 0
    
    for test_func in tests:
        try:
            test_func()
            passed += 1
        except AssertionError as e:
            print(f"âŒ Test Ã©chouÃ©: {e}")
            failed += 1
        except Exception as e:
            print(f"âŒ Erreur inattendue: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
    
    print("\n" + "=" * 70)
    print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
    print("=" * 70)
    print(f"âœ… Tests rÃ©ussis: {passed}/{len(tests)}")
    print(f"âŒ Tests Ã©chouÃ©s: {failed}/{len(tests)}")
    
    if failed == 0:
        print("\nğŸ‰ FÃ‰LICITATIONS! Tous les tests passent!")
        print("âœ… Votre rÃ´le de Data Officer est COMPLET et FONCTIONNEL")
        print("\nğŸ“ Vous pouvez maintenant:")
        print("   1. GÃ©nÃ©rer le dataset de test")
        print("   2. IntÃ©grer avec les autres rÃ´les")
        print("   3. Tester le systÃ¨me complet")
    else:
        print("\nâš ï¸  Certains tests ont Ã©chouÃ©")
        print("VÃ©rifiez les erreurs ci-dessus")
    
    print("=" * 70)
    
    return 0 if failed == 0 else 1


if __name__ == "__main__":
    sys.exit(main())

